{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4be1bb",
   "metadata": {},
   "source": [
    "# 3.4 softmax 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748f9e2",
   "metadata": {},
   "source": [
    "## 练习题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71b1a3",
   "metadata": {},
   "source": [
    "练习题 1：我们可以更深入地探讨指数族与softmax之间的联系\n",
    "\n",
    "* 计算 softmax 交叉熵损失 $l(\\mathbf{y},\\hat{\\mathbf{y}})$ 的二阶导数。\n",
    "\n",
    "* 计算 softmax(o) 给出的分布方差，并与上面计算的二阶导数匹配"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbfef9",
   "metadata": {},
   "source": [
    "$$\n",
    "l (y, \\hat{y}) = log \\sum^q_{k=1} exp(o_k) - \\sum^q_{j=1} y_j o_j\n",
    "\n",
    "$$\n",
    "\n",
    "一阶导：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l(y,\\hat{y})}{\\partial o_j} &= \\frac{e^{o_j}}{\\sum^q _{k=1} exp(o_k)} - y_j  \\\\\n",
    "&= softmax(o)_j - yj\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "基于求一阶导的中间式继续求二阶导：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 l(y,\\hat{y})}{\\partial o_j^2} &=  \\frac{e^{o_j}}{\\sum^q _{k=1} exp(o_k)} - [\\frac{e^{o_j}}{\\sum^q _{k=1} exp(o_k)}]^2  \\\\\n",
    "&= softmax(o)_j (1 - softmax(o)_j)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ef0573",
   "metadata": {},
   "source": [
    "计算方差：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "Var(o) &= \\frac{1}{q} \\sum_{k=1}^q (s_k - \\overline{s})^2   \\\\\n",
    "&= \\frac{1}{q} [(s_1 - \\overline{s})^2 + (s_2 - \\overline{s})^2 + ... + (s_q - \\overline{s})^2] \\\\\n",
    "&= \\frac{1}{q} [s_1^2 + s_2^2 + ... + s_1^2 + q \\overline{s}^2 - 2\\overline{s} (s_1 + s_2 + s_3 + ... + s_q)]   \\\\\n",
    "&= [\\sum_{k=1}^{q} s_k^2 - 2\\overline{s}\\sum_{k=1}^{q} s_k] + \\overline{s}^2    \\\\\n",
    "&= \\frac{1}{q} [\\sum_{k=1}^{q} s_k(s_k -1)] + \\overline{s} - \\overline{s}^2 \\\\\n",
    "&= -\\frac{1}{q} \\sum_{k=1}^{q} \\frac{\\partial^2 l}{\\partial o_k^2} + \\overline{s} - \\overline{s}^2\n",
    "\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8195b5",
   "metadata": {},
   "source": [
    "练习题 2：假设我们有三个类发生的概率相等，即概率向量是 $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n",
    "\n",
    "* 如果我们尝试为它设计二进制代码，有什么问题\n",
    "* 请设计一个更好的代码。提示：如果我们尝试编码两个独立的观察结果会发生什么？如果我们联合编码 n 个观测值怎么办"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07815b03",
   "metadata": {},
   "source": [
    "\n",
    "采用 one-hot 独热编码，分别设置为 001、010、100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d70193",
   "metadata": {},
   "source": [
    "练习题 3：softmax 是对上面介绍的映射的误称（虽然深度学习领域中很多人都使用这个名字）。真正的 softmax 被定义为\n",
    "$$\n",
    "\\mathrm{RealSoftMax}(a, b) = \\log (\\exp(a) + \\exp(b))\n",
    "$$\n",
    "\n",
    "* 证明 $\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$\n",
    "* 证明 $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$ 成立，前提是 $\\lambda > 0$\n",
    "* 证明对于 $\\lambda \\to \\infty$，有 $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$\n",
    "* soft-min 会是什么样子？\n",
    "* 将其扩展到两个以上的数字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5be140",
   "metadata": {},
   "source": [
    "* 证明 $\\mathrm{RealSoftMax}(a, b) > \\mathrm{max}(a, b)$：\n",
    "\n",
    "假设 $a >b$，则有：\n",
    "\n",
    "$$\n",
    "log(e^a + e^b) > log(e^a) = a\n",
    "$$\n",
    "\n",
    "同理可证明 $a < b$ 的情况，得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb14c27",
   "metadata": {},
   "source": [
    "* 证明 $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\mathrm{max}(a, b)$ 成立，前提是 $\\lambda > 0$\n",
    "\n",
    "假设 $a >b$，则有：\n",
    "\n",
    "$$\n",
    "\\lambda^{-1} log(e^{\\lambda a} + e^{\\lambda b}) > \\lambda^{-1} log e^{\\lambda a} = a\n",
    "$$\n",
    "\n",
    "同理可证明 $a < b$ 的情况，得证。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb431c64",
   "metadata": {},
   "source": [
    "* 证明对于 $\\lambda \\to \\infty$，有 $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$\n",
    "\n",
    "假设 $a > b$，由于 $\\lambda \\rightarrow \\infty$，所以有 $\\lambda a >> \\lambda b$，对于 $RealSoftMax(a,b) = log(e^a + e^b)$，在这样的前提下后面的 $e^b$ 可忽略不计。\n",
    "\n",
    "因此对于下限：\n",
    "\n",
    "$$\n",
    "\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) > \\lambda^{-1} log e^{\\lambda a} = a\n",
    "$$\n",
    "\n",
    "对于上限：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) &< \\lambda^{-1} log(e^{\\lambda a} + e^{\\lambda a})   \\\\\n",
    "\n",
    "&= \\lambda^{-1} [log2 + log e^{\\lambda a}]     \\\\\n",
    "&= \\frac{log2}{\\lambda} + a = a\n",
    "\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "由夹逼定理则可得到结果 $\\lambda^{-1} \\mathrm{RealSoftMax}(\\lambda a, \\lambda b) \\to \\mathrm{max}(a, b)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d0efe9",
   "metadata": {},
   "source": [
    "* soft-min 会是什么样子？\n",
    "\n",
    "$$\n",
    "\\mathrm{softmin}(x_i) = \\frac{e^{-x_i}}{\\sum^n_{j=1} e^{-xj}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534227c",
   "metadata": {},
   "source": [
    "* 将其扩展到两个以上的数字\n",
    "\n",
    "$$\n",
    "\\mathrm{RealSoftMax} (a,b,c, ....) = log (e^a + e^b + e^c + ...)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
